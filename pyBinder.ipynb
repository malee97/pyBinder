{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.Methods import *\n",
    "import os\n",
    "import platform\n",
    "from datetime import *\n",
    "import sys\n",
    "import warnings\n",
    "import psutil\n",
    "import traceback\n",
    "import csv\n",
    "import math\n",
    "import shutil \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.ticker as mticker\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from statsmodels.stats.libqsturng import qsturng, psturng\n",
    "import numpy as np\n",
    "import numpy_indexed as npi\n",
    "import pandas as pd\n",
    "from pyopenms import *\n",
    "from datetime import *\n",
    "import pandas as pd\n",
    "import pingouin as pg\n",
    "from hypothetical.descriptive import var\n",
    "from scipy import stats, signal\n",
    "from scipy.stats import zscore, t\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.integrate import quad, cumulative_trapezoid\n",
    "from scipy.signal import find_peaks, peak_widths, welch, savgol_filter\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import scikit_posthocs as sp\n",
    "from itertools import combinations\n",
    "import peakutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prots = ['Lumos','Eclipse']   # List of proteins in selection\n",
    "prots.sort()                                             # Alphabetizes all of the proteins for ease\n",
    "selection = 'PRTC Survey Lumos vs Eclipse'       \n",
    "# Name of selection, will be in folder name                                                             # E = Eclipse, L= Lumos, UT = Untargeted, T = Targeted\n",
    "enr_score_cutoff = 1/len(prots)                                                     # cutoff score for sequencing run\n",
    "p_score_cutoff = 0.15\n",
    "show_EICs = False\n",
    "nr_EICs = 100\n",
    "peak_RT = 300                                 # width (in seconds) of peak, used for max intensity search, gets divided by 2 for +-\n",
    "peps_sec = 3                    # maximum amount of peptides that can coelute, for building inclusion list\n",
    "reps = 10\n",
    "lib_size = 13\n",
    "conc_lib_start = 10    # pM/member\n",
    "PRTC_conc = 2*1000     # pM/member\n",
    "exclusion_list = []\n",
    "inclusion_list_easy = []\n",
    "inclusion_list_med = []\n",
    "inclusion_list_hard = []\n",
    "inclusion_lists = [inclusion_list_easy,inclusion_list_med,inclusion_list_hard]\n",
    "colors = ['r','y','g']\n",
    "\n",
    "    \n",
    "TIME = datetime.now().strftime(\"%Y_%m_%d-%I_%M_%S_%p\")                   # Gets current time to prevent duplicate folders        \n",
    "parent_dir = os.path.join('E:/','Eclipse_Data','20220718 - Lumos vs Eclipse PRTCs')\n",
    "folder = selection + ' ' + TIME                                        # Name of new folder to save output\n",
    "data_dir = os.path.join(parent_dir,folder)                               # New folder to store centroided data\n",
    "save_dir = os.path.join(parent_dir,folder)                               # If data is already centroided, use this folder \n",
    "                                                                         #    to save useful output\n",
    "default_thresh = 5E4                                                     # Threshold of peak detection (ion counts)\n",
    "LOD = 1.5e4\n",
    "reps = 4                                                                 # Number of replicates per protein\n",
    "centroided = False                                                       # Will skip the centroiding step if True\n",
    "check_data = False                                                        # Will skip the TIC and 2D plotting if False, prints out TIC otherwise\n",
    "full_out = True                                                          # Will print all features found for each protein\n",
    "known_binders = False                                                    # Do you have a formatted csv of known binders to find (for diagnostics)\n",
    "PRTC_check = True\n",
    "\n",
    "do_second_filter = True\n",
    "do_EICs = True                                                           # True to activate EICs, take majority of runtime\n",
    "do_subtract = False                                                      # Will subtract background from intensity for scoring\n",
    "use_savgol = True\n",
    "use_fft = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directory and reference setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_checks(parent_dir,prots,centroided,enr_score_cutoff,p_score_cutoff,peak_RT,peps_sec,reps,nr_EICs,LOD)\n",
    "eic_dirs,eic_dirs_spec,eic_dirs_nonspec,eic_dirs_other,png_dir,PRTC_dir = directory_setup(parent_dir,folder,save_dir,prots,centroided)\n",
    "\n",
    "if PRTC_check:\n",
    "    PRTC_masses = [900.5524,985.5220,1114.6374,1171.5861,1224.619,1488.7704]\n",
    "    PRTC_rts = [8.7*60,6.5*60,13.6*60,11.3*60,7.5*60,13.7*60]\n",
    "    PRTC_mass_round = np.round(PRTC_masses,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data centroiding and checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = data_centroiding(centroided,parent_dir,folder,PRTC_dir,data_dir)\n",
    "data_visualization(check_data,data_dir,png_dir,parent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = []\n",
    "feature_maps = []\n",
    "files = []\n",
    "files_full = []\n",
    "# paper on feature finder: https://doi.org/10.1021/pr300992u\n",
    "\n",
    "for file in sorted(os.listdir(data_dir)):\n",
    "    if file.endswith('.mzML'):\n",
    "        print(file)\n",
    "        if not 'PRTC_i' in file:\n",
    "            files.append(file[:-5])\n",
    "            files_full.append(file)\n",
    "        exp = MSExperiment()\n",
    "        out = FeatureXMLFile()\n",
    "        MzMLFile().load(os.path.join(data_dir,file),exp)\n",
    "        exp.updateRanges()\n",
    "        \n",
    "        feature_finder = FeatureFinder()       \n",
    "        # Get and set parameters \n",
    "        params = feature_finder.getParameters('centroided')\n",
    "        params.setValue('mass_trace:mz_tolerance',0.004) # default 0.004, old opt 0.004\n",
    "        params.setValue('isotopic_pattern:mz_tolerance',0.010) # default 0.005, old opt 0.010\n",
    "        params.setValue('isotopic_pattern:charge_low',2)\n",
    "        params.setValue('isotopic_pattern:charge_high',5)\n",
    "        params.setValue('feature:max_rt_span',3.0) # default 2.5 - try 1.5,2,3 - done, old opt 3.0\n",
    "        params.setValue('mass_trace:min_spectra',9) # default 10, try 9,8,7,6,5 - done, old opt 9 - 7 did nothing\n",
    "        params.setValue('feature:rt_shape', 'asymmetric')  # default symmetric - done, old opt asymmetric\n",
    "        params.setValue('seed:min_score',0.5)  # default 0.8, try 0.5,0.6,0.7 - done, old opt 0.5\n",
    "        params.setValue('feature:min_score',0.5) # default 0.7, try 0.6, 0.5 - done, old opt 0.5\n",
    "        params.setValue('mass_trace:max_missing',4)   # 1 is default, try 2,3,4,5 gave no features - done, old opt 4\n",
    "        print(params.items())\n",
    "    \n",
    "        # Run feature finder and store as featureXML and in array\n",
    "        if 'PRTC_i' in file:\n",
    "            PRTC_feature_map = FeatureMap()\n",
    "            feature_finder.run('centroided', exp, PRTC_feature_map, params, FeatureMap())\n",
    "            PRTC_feature_map.setPrimaryMSRunPath([str.encode(file[:-5])])\n",
    "            fXML = FeatureXMLFile()\n",
    "            file_name = file[:-5] + '.featureXML'\n",
    "            fXML.store(os.path.join(PRTC_dir,file_name),PRTC_feature_map)\n",
    "        else:\n",
    "            feature_map = FeatureMap()\n",
    "            feature_finder.run('centroided', exp, feature_map, params, FeatureMap())\n",
    "            feature_map.setPrimaryMSRunPath([str.encode(file[:-5])])\n",
    "            feature_maps.append(feature_map)\n",
    "            fXML = FeatureXMLFile()\n",
    "            file_name = file[:-5] + '.featureXML'\n",
    "            fXML.store(os.path.join(save_dir,file_name),feature_map)\n",
    "        print('feature map done')\n",
    "        print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature alignment and grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use replicate with most features as reference\n",
    "ref_index = [i[0] for i in sorted(enumerate([fm.size() for fm in feature_maps]), key=lambda x: x[1])][-1]\n",
    "ref_RTs = [f.getRT() for f in feature_maps[ref_index]]\n",
    "aligner = MapAlignmentAlgorithmPoseClustering()\n",
    "aligner.setReference(feature_maps[ref_index])\n",
    "align_params = aligner.getParameters()\n",
    "align_params.setValue('superimposer:mz_pair_max_distance', 0.5) \n",
    "align_params.setValue('pairfinder:distance_RT:max_difference',300.00)\n",
    "align_params.setValue('superimposer:max_shift',2000.0)\n",
    "aligner.setParameters(align_params)\n",
    "\n",
    "for feature_map in feature_maps[:ref_index] + feature_maps[ref_index + 1:]:\n",
    "    trafo = TransformationDescription()\n",
    "    aligner.align(feature_map, trafo)\n",
    "    transformer = MapAlignmentTransformer()\n",
    "    transformer.transformRetentionTimes(feature_map, trafo, True)  # store original RT as meta value\n",
    "\n",
    "new_RTs_list = []\n",
    "original_RTs_dict = {}\n",
    "\n",
    "for i,fm in enumerate(feature_maps):\n",
    "    if i == ref_index:\n",
    "        original_RTs_dict[i] = ref_RTs\n",
    "    else:\n",
    "        original_RT = [f.getMetaValue('original_RT') for f in fm]\n",
    "        original_RTs_dict[i] = original_RT\n",
    "\n",
    "alignment_dict = {}  # contains matrices of form [original_RT, aligned_RT]\n",
    "for i,dic in enumerate(original_RTs_dict.values()):\n",
    "    feature_map = feature_maps[i]\n",
    "    RTs_array = []\n",
    "    for j,RT_orig in enumerate(dic):\n",
    "        pair = [RT_orig,feature_map[j].getRT()]\n",
    "        RTs_array.append(pair)\n",
    "    alignment_dict[i] = RTs_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper on feature grouper: https://doi.org/10.1021/pr300992u\n",
    "\n",
    "feature_grouper = FeatureGroupingAlgorithmQT()  # Uses a quality threshold feature grouper\n",
    "grouper_params = feature_grouper.getParameters()\n",
    "grouper_params.setValue('distance_MZ:max_difference',0.01)\n",
    "grouper_params.setValue('distance_RT:max_difference',150.0)\n",
    "feature_grouper.setParameters(grouper_params)\n",
    "\n",
    "consensus_map = ConsensusMap()\n",
    "file_descriptions = consensus_map.getColumnHeaders()\n",
    "\n",
    "for i, feature_map in enumerate(feature_maps):\n",
    "    file_description = file_descriptions.get(i, ColumnHeader())\n",
    "    file_description.filename = feature_map.getMetaValue('spectra_data')[0].decode()\n",
    "    file_description.size = feature_map.size()\n",
    "    file_description.unique_id = feature_map.getUniqueId()\n",
    "    file_descriptions[i] = file_description\n",
    "    \n",
    "consensus_map.setColumnHeaders(file_descriptions)\n",
    "feature_grouper.group(feature_maps, consensus_map)\n",
    "\n",
    "ConsensusXMLFile().store(os.path.join(save_dir,'consensusmap.consensusXML'),consensus_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRTC_features_mz = []\n",
    "PRTC_features_RT = []\n",
    "for f in PRTC_feature_map:\n",
    "    obs_mass = (f.getMZ()*f.getCharge())-1.0074*f.getCharge()\n",
    "    if np.round(obs_mass,2) in PRTC_mass_round and f.getCharge() == 2:\n",
    "        PRTC_features_mz.append(f.getMZ())\n",
    "        PRTC_features_RT.append(f.getRT())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_feat = []               \n",
    "for cf in consensus_map:\n",
    "    RT_cf = cf.getRT()\n",
    "    Qual_cf = cf.getQuality()\n",
    "    Charge_cf = cf.getCharge()\n",
    "    MZ_cf = cf.getMZ()\n",
    "    con_feat.append([cf.getRT(),cf.getCharge(),cf.getMZ(),cf.getQuality()])\n",
    "    \n",
    "map_idxs = []\n",
    "map_RTs = []\n",
    "\n",
    "for cf in consensus_map:\n",
    "    map_idx = []\n",
    "    map_RT = []\n",
    "    for f in cf.getFeatureList():\n",
    "        map_idx.append(f.getMapIndex())\n",
    "        map_RT.append(f.getRT())\n",
    "    map_idxs.append(map_idx)\n",
    "    map_RTs.append(map_RT)\n",
    "\n",
    "new_map_idxs = []\n",
    "new_map_RTs = []   # these are the RTs from each feature map post-alignment\n",
    "\n",
    "for i in range(len(map_RTs)):\n",
    "    feat_idxs = map_idxs[i]\n",
    "    feat_RTs = map_RTs[i]\n",
    "    new_map_idx = []\n",
    "    new_map_RT = []\n",
    "    for i in range(len(feature_maps)):\n",
    "        new_map_idx.append(i)\n",
    "        if i in feat_idxs:\n",
    "            RT = feat_RTs[feat_idxs.index(i)]\n",
    "            new_map_RT.append(RT)\n",
    "        if i not in feat_idxs:\n",
    "            new_map_RT.append(0)   # this is where the problem is - if feature not in replicate, puts 0 for RT\n",
    "    new_map_idxs.append(new_map_idx)\n",
    "    new_map_RTs.append(new_map_RT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alignment checks, original RT extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_RT_maps_formatted = alignment_check(consensus_map,feature_maps,png_dir,new_map_RTs,files,original_RTs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = []\n",
    "for fm in feature_maps:\n",
    "    col_names.append(fm.getMetaValue('spectra_data')[0].decode())\n",
    "\n",
    "cf_df_1 = pd.DataFrame(con_feat,columns=['RT consensus','charge','m/z','quality'])\n",
    "cf_df_3 = pd.DataFrame(fixed_RT_maps_formatted,columns=col_names)\n",
    "cf_df_2 = pd.DataFrame(new_map_RTs,columns=col_names)\n",
    "\n",
    "cf_df_final = pd.concat([cf_df_1,cf_df_2,cf_df_3],axis=1)\n",
    "cf_df_final_np = cf_df_final.to_numpy()\n",
    "\n",
    "cf_df_final.to_csv(os.path.join(parent_dir,folder,'Consensus Features.csv'),index=False)\n",
    "\n",
    "feat_RT_cf_combined = []\n",
    "feat_mz_combined = []\n",
    "feat_z_combined = []\n",
    "feat_quality_combined = []\n",
    "feat_obs_mass_combined = []\n",
    "RTs = []\n",
    "\n",
    "for entry in cf_df_final_np:\n",
    "    feat_RT_cf_combined.append(entry[0])\n",
    "    feat_mz_combined.append(entry[2])\n",
    "    feat_z_combined.append(entry[1])\n",
    "    feat_obs_mass_combined.append(entry[2]*entry[1]-entry[1]*1.0074)\n",
    "    feat_quality_combined.append(entry[3])\n",
    "    RTs.append(entry[4:])\n",
    "    \n",
    "print(len(feat_mz_combined))\n",
    "    \n",
    "feat_RT_cf_combined_massfiltered = []\n",
    "feat_RT_orig_massfiltered = []\n",
    "feat_mz_combined_massfiltered = []\n",
    "feat_z_combined_massfiltered = []\n",
    "feat_quality_combined_massfiltered = []\n",
    "feat_obs_mass_combined_massfiltered = []\n",
    "RTs_massfiltered = []\n",
    "\n",
    "ref_feature_mass = []\n",
    "ref_feature_RT = []\n",
    "ref_feature_RT_orig = []\n",
    "refs_found = []\n",
    "\n",
    "for i,obs_mass in enumerate(feat_obs_mass_combined):\n",
    "    entry = cf_df_final_np[i]\n",
    "    if np.round(obs_mass,2) in PRTC_mass_round and entry[1] == 2:\n",
    "        ref_feature_mass.append(entry[2])\n",
    "        ref_feature_RT.append(entry[0])\n",
    "        ref_feature_RT_orig.append(fixed_RT_maps_formatted[i])\n",
    "        refs_found.append(np.round(obs_mass,2))\n",
    "    elif obs_mass > 57.021*lib_size + 17.98:   # assuming we have a peptide of the correct length of only glycines\n",
    "        feat_RT_cf_combined_massfiltered.append(entry[0])\n",
    "        feat_RT_orig_massfiltered.append(fixed_RT_maps_formatted[i])\n",
    "        feat_mz_combined_massfiltered.append(entry[2])\n",
    "        feat_z_combined_massfiltered.append(entry[1])\n",
    "        feat_obs_mass_combined_massfiltered.append(entry[2]*entry[1]-entry[1]*1.0074)\n",
    "        feat_quality_combined_massfiltered.append(entry[3])\n",
    "        RTs_massfiltered.append(entry[4:])\n",
    "    \n",
    "for mass in PRTC_mass_round:\n",
    "    if mass not in refs_found:\n",
    "        warnings.warn(f\"Warning: PRTC reference mass {mass} not found in consensus features!\")\n",
    "print(len(feat_mz_combined_massfiltered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mzs_list,RTs_list,ints_list = get_data(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get PRTC peptide areas from initial reference\n",
    "mzs_ref,RTs_ref,ints_ref = get_data(PRTC_dir)\n",
    "_,rt_windows_start,int_windows_start,_ = feature_int_extractor_start(PRTC_features_mz,PRTC_features_RT,mzs_ref,RTs_ref,ints_ref,LOD=1.5E4,noise_level=1.5e4,peak_range=peak_RT)\n",
    "areas_PRTC = [val[0] for val in feature_area_extractor_savgol(rt_windows_start,int_windows_start)]\n",
    "\n",
    "PRTCs_found_start = np.unique(np.round(PRTC_features_mz,1))\n",
    "PRTC_area_ref = {}\n",
    "for mass in PRTCs_found_start:\n",
    "    idxs = np.where(np.round(PRTC_features_mz,1) == mass)[0]\n",
    "    area_ion = np.average([np.average(entry) for entry in [areas_PRTC[j] for j in idxs if areas_PRTC[j] > 1]])\n",
    "    PRTC_area_ref[mass] = area_ion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get PRTC peptide areas within replicates\n",
    "ints_ref,rt_windows_ref,int_windows_ref,backs_ref = feature_int_extractor(ref_feature_mass,ref_feature_RT,ref_feature_RT_orig,mzs_list,RTs_list,ints_list,LOD=1.5E4,noise_level=1.5e4,peak_range=peak_RT)\n",
    "areas_ref_full = feature_area_extractor_savgol(rt_windows_ref,int_windows_ref)\n",
    "areas_ref_average = [np.average(entry) for entry in areas_ref_full]\n",
    "area_ref = np.average(areas_ref_average)*conc_lib_start/PRTC_conc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRTC_stats(areas_ref_full,files,ref_feature_mass,png_dir)\n",
    "\n",
    "PRTCs_found = np.unique(np.round(ref_feature_mass,1))\n",
    "PRTC_area_reps = {}\n",
    "for mass in PRTCs_found:\n",
    "    idxs = np.where(np.round(ref_feature_mass,1) == mass)[0]\n",
    "    area_average_ion = np.average([np.average(entry) for entry in [[area for area in areas_ref_full[j] if area > 1] for j in idxs]])\n",
    "    PRTC_area_reps[mass] = area_average_ion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ints,rt_windows, int_windows, backgrounds = feature_int_extractor(feat_mz_combined_massfiltered,feat_RT_cf_combined_massfiltered,feat_RT_orig_massfiltered,mzs_list,RTs_list,ints_list,LOD=1.5E4,noise_level=1.5E4,peak_range=peak_RT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_savgol:\n",
    "    peak_regular = peak_finder_savgol(rt_windows,int_windows,prots,plot=False,default_thresh=default_thresh,kernal_size=3,width=5,rel_height=0.5,reps=reps)\n",
    "    peak_filtered = []\n",
    "    feat_mz_filtered = []\n",
    "    feat_RT_filtered = []\n",
    "    feat_RT_orig_filtered = []\n",
    "    feat_z_filtered = []\n",
    "    check_rt_windows = []\n",
    "    check_int_windows = []\n",
    "    max_ints_filtered = []\n",
    "    for i,peak_group in enumerate(peak_regular):\n",
    "        test = np.concatenate(peak_group)\n",
    "        if np.any(test):\n",
    "            peak_filtered.append(peak_group)\n",
    "            feat_mz_filtered.append(feat_mz_combined_massfiltered[i])\n",
    "            feat_RT_filtered.append(feat_RT_cf_combined_massfiltered[i])\n",
    "            feat_RT_orig_filtered.append(feat_RT_orig_massfiltered[i])\n",
    "            check_rt_windows.append(rt_windows[i])\n",
    "            check_int_windows.append(int_windows[i])\n",
    "            feat_z_filtered.append(feat_z_combined_massfiltered[i])\n",
    "            max_ints_filtered.append(max_ints[i])\n",
    "\n",
    "    print(f'Features removed: {len(feat_mz_combined_massfiltered) - len(feat_mz_filtered)}')\n",
    "    print(f'Features remaining: {len(feat_mz_filtered)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas_savgol = feature_area_extractor_savgol(check_rt_windows,check_int_windows,check=False,width_start=10,prominence=2,threshold=10,rel_height=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enrichmentscores,enrichment_normalized,pvals,specifics,spec_label = enr_scoring(areas_savgol,area_ref,prots,reps=reps,p_score_cutoff=p_score_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not do_second_filter:\n",
    "    feat_mz_filtered = feat_mz_combined_massfiltered\n",
    "    feat_RT_filtered = feat_RT_cf_combined_massfiltered\n",
    "    feat_z_filtered = feat_z_combined_massfiltered\n",
    "    max_ints_filtered = max_ints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_graphing,es_normalized_graphing,ps_graphing,RTs_graphing,RTs_graphing_orig,mzs_graphing,z_graphing,areas_graphing,es_nonspecific,es_normalized_nonspecific,ps_nonspecific,RTs_nonspecific,RTs_nonspecific_orig, \\\n",
    "    mzs_nonspecific,z_nonspecific,areas_nonspecific, es_unclear,es_normalized_unclear,ps_unclear,RTs_unclear,RTs_unclear_orig,mzs_unclear,z_unclear,areas_unclear = setup_output(prots,spec_label,enrichmentscores,enrichment_normalized,pvals,feat_RT_filtered,feat_RT_orig_filtered,feat_mz_filtered,feat_z_filtered,areas_savgol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enrichment_rankings(prots,es_graphing,es_normalized_graphing,png_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volcano_plotting(prots,es_graphing,es_nonspecific,es_unclear,ps_graphing,ps_nonspecific,ps_unclear,inclusion_lists,feat_mz_combined,enrichmentscores,p_plot,colors,p_score_cutoff,enr_score_cutoff,save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volcano_plots_normalized(prots,es_normalized_graphing,es_normalized_nonspecific,es_normalized_unclear,ps_graphing,ps_nonspecific,ps_unclear,p_score_cutoff,enr_score_cutoff,save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_results(enrichmentscores,es_graphing,RTs_graphing,mzs_graphing,ps_graphing,z_graphing,enr_score_cutoff,p_score_cutoff,prots,parent_dir,folder,folder,feat_RT_cf_combined,feat_mz_filtered,feat_z_filtered,pvals,spec_label,areas_savgol,full_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_known_binders(known_binders,prots,parent_dir,folder,feat_mz_combined_massfiltered,feat_mz_combined,feat_RT_cf_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EICs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_EICs:\n",
    "    plot_PRTC(ref_feature_mass,ref_feature_RT,ref_feature_RT_orig,areas_ref_full,data_dir,PRTC_dir,baseline=0,peak_range=peak_RT,show_plot=show_EICs,reps=reps)\n",
    "    \n",
    "    for i,score in enumerate(es_graphing):\n",
    "        score_sorted = np.sort(score)[::-1]\n",
    "        indices_sorted = np.argsort(score)[::-1]\n",
    "        count = 0\n",
    "        mzs = []\n",
    "        RTs = []\n",
    "        RTs_orig = []\n",
    "        scores = []\n",
    "        p_to_graph = []\n",
    "        areas_to_graph = []\n",
    "        while score_sorted[count] >= enr_score_cutoff and len(scores) < nr_EICs:\n",
    "            index = indices_sorted[count]\n",
    "            mzs.append(mzs_graphing[i][index])\n",
    "            RTs.append(RTs_graphing[i][index])\n",
    "            RTs_orig.append(RTs_graphing_orig[i][index])\n",
    "            scores.append(score_sorted[count])\n",
    "            p_to_graph.append(ps_graphing[i][index][0])\n",
    "            areas_to_graph.append(areas_graphing[i][index])\n",
    "            count -= -1\n",
    "            if count >= len(score):\n",
    "                break\n",
    "        print(' ')\n",
    "        specific = plot_EIC(mzs,RTs,RTs_orig,scores,p_to_graph,areas_to_graph,data_dir,eic_dirs_spec[i],baseline=0,peak_range=peak_RT,show_plot=show_EICs,reps=reps)\n",
    "    if es_nonspecific:\n",
    "        for i in range(len(prots)):\n",
    "            score = [entry[i] for entry in es_nonspecific]\n",
    "            score_sorted = np.sort(score)[::-1]\n",
    "            indices_sorted = np.argsort(score)[::-1]\n",
    "            count = 0\n",
    "            mzs = []\n",
    "            RTs = []\n",
    "            RTs_orig = []\n",
    "            scores = []\n",
    "            p_to_graph = []\n",
    "            areas_to_graph = []\n",
    "            while score_sorted[count] >= enr_score_cutoff and len(scores) < nr_EICs:\n",
    "                index = indices_sorted[count]\n",
    "                mzs.append(mzs_nonspecific[index])\n",
    "                RTs.append(RTs_nonspecific[index])\n",
    "                RTs_orig.append(RTs_nonspecific_orig[index])\n",
    "                scores.append(score_sorted[count])\n",
    "                p_to_graph.append(ps_nonspecific[index][0])\n",
    "                areas_to_graph.append(areas_nonspecific[index])\n",
    "                count -= -1\n",
    "                if count >= len(score):\n",
    "                    break\n",
    "            print(' ')\n",
    "            nonspecific = plot_EIC(mzs,RTs,RTs_orig,scores,p_to_graph,areas_to_graph,data_dir,eic_dirs_nonspec[i],baseline=0,peak_range=peak_RT,show_plot=show_EICs,reps=reps)\n",
    "    for i in range(len(prots)):\n",
    "        score = [entry[i] for entry in es_unclear]\n",
    "        score_sorted = np.sort(score)[::-1]\n",
    "        indices_sorted = np.argsort(score)[::-1]\n",
    "        count = 0\n",
    "        mzs = []\n",
    "        RTs = []\n",
    "        RTs_orig = []\n",
    "        scores = []\n",
    "        p_to_graph = []\n",
    "        areas_to_graph = []\n",
    "        while len(scores) < nr_EICs:\n",
    "            index = indices_sorted[count]\n",
    "            mzs.append(mzs_unclear[index])\n",
    "            RTs.append(RTs_unclear[index])\n",
    "            RTs_orig.append(RTs_unclear_orig[index])\n",
    "            scores.append(score_sorted[count])\n",
    "            p_to_graph.append(ps_unclear[index][0])\n",
    "            areas_to_graph.append(areas_unclear[index])\n",
    "            count -= -1\n",
    "            if count >= len(score):\n",
    "                break\n",
    "        print(' ')\n",
    "        unclear = plot_EIC(mzs,RTs,RTs_orig,scores,p_to_graph,areas_to_graph,data_dir,eic_dirs_other[i],baseline=0,peak_range=peak_RT,show_plot=show_EICs,reps=reps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c3b898a23a1875192910a6e0e22ef616011573b27e9185b59c9b176d8e940298"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
